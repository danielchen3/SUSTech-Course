# CS323 Compiler Final Review

## Introduction

1. High-Level language <- Low-Level languageï¼Œæœºå™¨è¯­è¨€ -> æ±‡ç¼–è¯­è¨€ -> é«˜çº§è¯­è¨€

   ä¸€äº›æ—©æœŸé«˜çº§è¯­è¨€çš„ç‰¹ç‚¹ï¼š

   1. Fortran(First High level language): for scientific computation 
   2. Cobol: for business data processing 
   3. Lisp: for symbolic computation

2. Compiler Structure ç¼–è¯‘å™¨çš„ç»“æ„

   Source code -> å‰ç«¯ï¼ˆLexical, syntax, Semantic Analysis and Intermediate code generatorï¼‰-> IR

   IR -> åç«¯ï¼ˆMachine-Independent Code Optimizer, Code Generator, Machine-Dependent Code Optimizerï¼‰-> æœºå™¨è¯­è¨€

   - è¯æ³•åˆ†æï¼Œç”Ÿæˆtoken stream. 
     - Lexemeè¯ç´ æ˜¯æºä»£ç ä¸­å…·æœ‰æŸç§ç‰¹å®šæ„ä¹‰çš„æœ€å°å•å…ƒï¼Œå®ƒæ˜¯ç¨‹åºæ–‡æœ¬ä¸­çš„å®é™…å­—ç¬¦åºåˆ—ï¼Œè¡¨ç¤ºæŸä¸ªè¯­è¨€æ„é€ çš„å…·ä½“å®ä¾‹(Instance)
     - Token è¯æ³•å•å…ƒ è¯æ³•å•å…ƒæ˜¯è¯ç´ åœ¨è¯æ³•åˆ†æè¿‡ç¨‹ä¸­è¢«åˆ†ç±»åçš„æŠ½è±¡ç¬¦å·æˆ–æ ‡ç­¾(pattern) <token_name, attribute_value>
   - è¯­æ³•åˆ†æï¼Œç”Ÿæˆè¯­æ³•æ ‘
   - è¯­ä¹‰åˆ†æï¼Œä¸»è¦è¿›è¡Œç±»å‹æ£€æŸ¥ã€è½¬æ¢
   - ä¸­é—´ä»£ç ç”Ÿæˆï¼Œç”ŸæˆIR(typically ä¸‰åœ°å€ç )
   - Machine-Independent Code Optimizer æœºå™¨æ— å…³ä»£ç ä¼˜åŒ–ï¼Œè¾“å‡ºä¼˜åŒ–ä¹‹åçš„IR
   - ä»£ç ç”Ÿæˆ Code generationï¼Œç”Ÿæˆç›®æ ‡ä»£ç target code

3. Compiler VS Interpreter

   Compiler translates source code in high-level language -> machine code.

   Interpreter directly execute source code without compiling.

   ç”¨è§£é‡Šå™¨è§£é‡Šçš„ç¼–ç¨‹è¯­è¨€åœ¨ç¢°åˆ°ç¬¬ä¸€ä¸ªerroræ—¶å€™åœæ­¢ï¼Œç”¨ç¼–è¯‘å™¨çš„åœ¨å…¨éƒ¨ç¼–è¯‘æˆåŠŸä¹‹åå¼€å§‹è¿è¡Œ



## Lexical Analysis ï¼ˆè¯æ³•åˆ†æï¼‰

1. æ­£åˆ™è¡¨è¾¾å¼

   1. prefix, proper prefix, suffix, proper suffix(ä¸æ˜¯ç©ºé›†ä¸”ä¸ç­‰äºè‡ªå·±)
   2. substring, proper substring, subsequence, string concatenation, exponentiation
   3. A language is any countable set1 of strings over some fixed alphabet
   4. å¹¶ï¼Œè¿æ¥ï¼ŒKleeneé—­åŒ…($a^*$)ï¼Œæ­£é—­åŒ…($a^{+}$)
   5. Precedence (ä¼˜å…ˆçº§): closure * > concatenation > union  | AND  left associative
   6. r? = r | $\epsilon$

2. æœ‰ç©·è‡ªåŠ¨æœº

   - NFA éç¡®å®šæœ‰ç©·è‡ªåŠ¨æœº(S, $\sum$(input string), start state, transition function, accept state)

   - DFA ç¡®å®šæœ‰ç©·è‡ªåŠ¨æœº for {s,a}, exactly one edge out

   - NFA -> DFA

     â˜…å…³é”®ç®—æ³• Subset Construction Technique å­é›†æ„é€ æ³•	

     - ğœ–-closure(s) ï¼Œğœ–-closure(T)ï¼Œmove(T,a)
     - æ‰¾çŠ¶æ€é›†åˆï¼Œç„¶åæ‰¾ç©ºé›†é—­åŒ…ï¼Œè®¾ä¸ºæ–°çŠ¶æ€ï¼Œä»æ–°çŠ¶æ€å‡ºå‘å¯¹æ¯ä¸ªinputæ±‚æ–°çš„é›†åˆç„¶åå†æ±‚é—­åŒ…

   - æ­£åˆ™è¡¨è¾¾å¼ -> NFA

     â˜… å…³é”®ç®—æ³• Thompson's construction algorithm

     å‡ºç°conflictå…ˆæ¥å—å…ˆspecifyçš„



## Syntax Analysisï¼ˆè¯­æ³•åˆ†æï¼‰

1. Parser åˆ†ç±»

   Universal parsers (é€šç”¨è¯­æ³•åˆ†æå™¨) Some methods (e.g., Earleyâ€™s algorithm1) can parse any grammar â–ª However, they are too inefficient to be used in practice 

   Top-down parsers (è‡ªé¡¶å‘ä¸‹è¯­æ³•åˆ†æå™¨) â–ª Construct parse trees from the top (root) to the bottom (leaves) 

   Bottom-up parsers (è‡ªåº•å‘ä¸Šè¯­æ³•åˆ†æå™¨)

2. CFGå‡ ä¸ªé‡è¦çš„éƒ¨åˆ†ï¼ˆterminal, non-terminal, start symbol, productionï¼‰

3. ä¸Šä¸‹æ–‡æ— å…³æ–‡æ³•æ˜¯æè¿°è¯­è¨€ç”Ÿæˆè§„åˆ™çš„å·¥å…·ï¼Œè€Œä¸Šä¸‹æ–‡æ— å…³è¯­è¨€æ˜¯é€šè¿‡è¿™äº›è§„åˆ™ç”Ÿæˆçš„è¯­è¨€ã€‚

4. derivations(left-most one-to-one parser trees; right-most many-to-one parser trees)

5. sentential form(æ–‡æ³•çš„å¥å‹) æ˜¯æ¨å¯¼ä¸­å¯èƒ½å‡ºç°çš„è¡¨è¾¾å½¢å¼ï¼Œå…¶ä¸­å¯èƒ½åŒ…æ‹¬non-terminal

6. sentence(å¥å­) æ˜¯æœ€ç»ˆè¾“å‡ºï¼Œå…¶ä¸­éƒ½å¿…é¡»æ˜¯terminal

7. parse tree æ˜¯å¯¹derivationçš„è¡¨ç¤ºï¼Œè€ŒAST(æŠ½è±¡è¯­æ³•æ ‘)æ˜¯å¯¹æºä»£ç çš„æŠ½è±¡ç»“æ„ï¼Œä¸å…³æ³¨ç»†èŠ‚

8. æ–‡æ³•çš„ambiguityï¼Œå³å¯¹ä¸€ä¸ªsentenceå¯ä»¥æœ‰å¤šä¸ªparse treeï¼Œå°±ä»£è¡¨æ–‡æ³•å­˜åœ¨äºŒä¹‰æ€§

9. æ­£åˆ™è¡¨è¾¾å¼(regular expression) è¡¨è¾¾èƒ½åŠ›å°äº CFGï¼Œå³æ‰€æœ‰å¯ä»¥ç”±REè¡¨ç¤ºçš„éƒ½å¯ä»¥ç”±CFGè¡¨ç¤º

10. è¯­æ³•åˆ†ææŠ€æœ¯

    - è‡ªé¡¶å‘ä¸‹æ–¹æ³•(Top-down parser) predict-match, leftmost è¦æ±‚æ–‡æ³•ä¸ºLL(1) å¦åˆ™éœ€è¦å¤„ç†å†²çª

      1. Recursive-descent parsing

         Recursive-descent parsers needing no backtracking can be constructed for a class of grammars called LL(1)

         å®ƒç›´æ¥ä½¿ç”¨æ–‡æ³•çš„äº§ç”Ÿå¼æ¥æ„é€ é€’å½’è°ƒç”¨ã€‚æ¯ä¸ªæ–‡æ³•è§„åˆ™éƒ½å¯¹åº”ä¸€ä¸ªå‡½æ•°ï¼ˆæˆ–è¿‡ç¨‹ï¼‰ï¼Œè¯¥å‡½æ•°ä¼šæ ¹æ®è¾“å…¥å­—ç¬¦ä¸²ä¸­çš„å­—ç¬¦æ¥è¿›è¡Œé€’å½’è°ƒç”¨

      2. Non-recursive predictive parsing

         **Table-Driven Predictive Parsing**(é€’å½’å‘ä¸‹å¯»æ‰¾è¡¨è¾¾å¼æ›¿ä»£ FIRST FOLLOWé›†åˆçš„æ„é€ ï¼Œæ„é€ é¢„æµ‹åˆ†æè¡¨ï¼Œå¦‚æœæ²¡æœ‰å†²çªåˆ™æ˜¯LL(1))

    - è‡ªåº•å‘ä¸Šæ–¹æ³•(Bottom-up parser) shift-reduce, rightmost

      ç›¸å¯¹äºLL LRçš„å¥½å¤„å¦‚ä¸‹

      1. Table-driven (like non-recursive LL parsers) and powerful
      2. LR-parsing is the most general nonbacktracking shift-reduce parsing method known
      3. LR grammars can describe more languages than LL grammars

      éœ€è¦å®šä¹‰

      1. Augumented grammar
      2. two funtions : CLOSURE, GOTO

      LR Parsing Table: ACTION + GOTO

      1. SLR åŸºäº LRï¼ˆ0ï¼‰
      
         éœ€è¦æŒæ¡å…·ä½“ç®—æ³•ç»†èŠ‚â˜†

      2. CLR åŸºäº LRï¼ˆ1ï¼‰
      
      3. LALR
      
      è¡¨è¾¾çš„è¯­è¨€CLR > LALR > SLR  stateä¸ªæ•° CLR > LALR = SLR Driver programs å‡ç›¸ç­‰



## Syntax-Directed Translation(è¯­æ³•åˆ¶å¯¼çš„ç¿»è¯‘)

### SDD Introduction

1. Definition : A syntax-directed definition (è¯­æ³•åˆ¶å¯¼å®šä¹‰,SDD) is a context free grammar together with attributes and rules

   Attributeï¼ˆå±æ€§ï¼‰A set of attributes (å±æ€§) is associated with each grammar symbol

   Semantic ruleï¼ˆè¯­ä¹‰è§„åˆ™ï¼‰is associated with a production and describes how attributes are computed

2. åˆæˆå±æ€§(Synthesized Attribute) åˆæˆå±æ€§æ˜¯ä»å­èŠ‚ç‚¹ä¼ é€’åˆ°çˆ¶èŠ‚ç‚¹çš„å±æ€§ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåˆæˆå±æ€§çš„è®¡ç®—ä¾èµ–äºå­èŠ‚ç‚¹çš„å€¼ï¼Œè€Œè®¡ç®—çš„ç»“æœå°†ä¼ é€’ç»™çˆ¶èŠ‚ç‚¹ã€‚ å¯ä»¥åœ¨Botton-up parserä¸­ç›´æ¥è·å–

3. ç»§æ‰¿å±æ€§(Inherited Attribute)  Inherited attributes have their value at a parse-tree node determined from attribute values at the **node itself, its parent, and its siblings** in the parse tree



### Evaluation Orders for SDDâ€™

Dependency graphï¼ˆä¾èµ–å›¾ï¼‰æ ¹æ®è¯­ä¹‰åŠ¨ä½œè¿æ¥ï¼Œç„¶åTOPOæ ‡åºå·

1. S-attributed SDD An SDD is S-attributed if every attribute is synthesized

   So, S-attributed SDDs can be easily implemented during  bottom-up parsing (using ååºéå†)

2. L-attributed SDD

   An SDD is L-attributed if for each production ğ´ â†’ ğ‘‹1ğ‘‹2â€¦ğ‘‹ğ‘›, for  each ğ‘— = 1â€¦ğ‘›, each inherited attribute of ğ‘‹ğ‘— depends on only: the attributes of ğ‘‹1,â€¦,ğ‘‹ğ‘—âˆ’1 (either synthesized or inherited ), or  the inherited attributes of A

   OR it's a S-attributed SDD

   Dependency-graph edges can go from left to right (on  an annotated parse tree), but not right to left (hence the  SDD is named â€œL-attributedâ€)
   
   S-attributed SDD æ˜¯ L-attributed SDDçš„å­é›†
   
   ä½¿ç”¨dfséå†ï¼Œéå†æ—¶å€™è®°å¾—ä»å·¦è¾¹å‘å³è¾¹ï¼Œå› ä¸ºè¿™æ ·å¯ä»¥ä½¿å¾—å·¦è¾¹çš„siblingså·²ç»è¢«æå‰evaluated



### Syntax-Directed Translation Schemes

SDT -> Semantic action(Real code in {}) WHILE SDD -> Semantic rule(Mathmetical definitions)

SDT sometimes can be implemented during parsing without first building a parse tree

å¦‚ä½•determineèƒ½å¦åšåˆ°è¿™ä¸€ç‚¹å‘¢? é€šè¿‡å¼•å…¥æ ‡è®°éç»ˆç»“ç¬¦M and M â†’ Îµ  for embedded action å¦‚æœæ ‡è®°è¯¥æ–‡æ³•èƒ½å¤Ÿè¢«è¯¥è§£æä¹‹åæ–¹æ³•æ— å†²çªæœ‰æ•ˆå¤„ç†ï¼Œé‚£ä¹ˆè¯´æ˜é€šè¿‡è¿™ç§æ–¹æ³•èƒ½å¤Ÿ **åœ¨è§£æè¿‡ç¨‹ä¸­å®ç° SDT**4

**SDT**çš„æ„é€ 

<img src="image-20250103153305348.png" alt="image-20250103153305348" style="zoom:25%;" />



## Intermediate-Code Generation(ä¸­é—´ä»£ç ç”Ÿæˆ)

IRæ˜¯å‰ç«¯çš„æœ€åæ­¥éª¤

### Intermediate Representation

1. Constructing DAG

2. **Three-Address Code**

   A variable has l-value and r-value:  L-value (location) AND R-value (content)

   Three typical representations

   1.  Quadruples (å››å…ƒå¼è¡¨ç¤ºæ–¹æ³•) op arg1 arg2 result

   2. Triples (ä¸‰å…ƒå¼è¡¨ç¤ºæ–¹æ³•)  op, arg1, arg2 arg2å¤„æ”¾ç½®çš„é€šå¸¸æ˜¯ä¹‹å‰è¡¨è¾¾å¼çš„è¡Œå·ï¼Œç›¸å½“äºå°±æ˜¯result

      without generating temporary names  **(an optimization over quadruples)**

      **BUT** åœ¨ä¼˜åŒ–æ—¶å€™ï¼Œé€šå¸¸ä¼šæ¶‰åŠåˆ°ä½ç½®äº¤æ¢ï¼Œè¿™æ ·Tripleå°±ä¼šå—åˆ°å½±å“ï¼Œä½†æ˜¯Quaå°±ä¸ä¼šæœ‰å½±å“

   3. Indirect triples (é—´æ¥ä¸‰å…ƒå¼è¡¨ç¤ºæ–¹æ³•)

      ä¸€ä¸ªæå‡å°±æ˜¯ It consists of a list of pointers to tripleï¼Œè¿™å°±åœ¨re-orderingæ—¶å€™å¾ˆæ–¹ä¾¿

   Static single-assignment form (SSA, é™æ€å•èµ‹å€¼å½¢å¼) ç”¨x1,x2....

### Type and Declarations

1. Type Expression

   Types have structure, which can be represented by type expressions : A type expression is either a basic type, OR Formed by applying a type constructor (ç±»å‹æ„é€ ç®—å­) to a type expression

   basic type OR type name(self-constructed)

2. Type Equivalence

   - Name Equivalence(åç­‰ä»·) 

     Two type expressions are name equivalent if and only if they are identical  (represented by the same syntax tree, with the same labels)

   - Structural Equivalence(ç»“æ„ç­‰ä»·)

      For named types, replace the names by the type expressions and recursively check the substituted trees

3. **Translation Process Example** detailed please refer to lec05-slide40



### Type Checking

A language is strongly typed if the compiler guarantees that the  programs it accepts will run **without type errors**(sound type system) Strong typed(Java...)  Weakly typed(C/C++...)

ç±»å‹åˆæˆ && ç±»å‹æ¨å¯¼

**Type Conversion** ç±»å‹è½¬æ¢

1. Widening conversion ä¿ç•™ä¿¡æ¯ï¼Œç›¸å½“äºimplicitly type conversion å¯ä»¥ç”±compilerè‡ªåŠ¨å®Œæˆ
2. Narrowing conversion ä¸¢å¤±ä¿¡æ¯ï¼Œç›¸å½“äºexplicitly type conversion éœ€è¦ç”±programmaræ‰‹åŠ¨å®Œæˆ
3. SDTè¿‡ç¨‹ä¸­æœ‰ä¸¤ä¸ªå‡½æ•°æ¥å¤„ç†(é’ˆå¯¹widening)
   - max(t1,t2) è¿”å›least upper bound.
   - widen(a,t,w) generates type conversions if needed to widen an address ğ‘ of type ğ‘¡ into a value of type w.



### Arrays Addresses

1. ä¸€ç»´æ•°ç»„ ğ’ƒğ’‚ğ’”ğ’†+ğ’Šâˆ—ğ’˜ (ğ‘ğ‘ğ‘ ğ‘’ is the relative address of ğ´[0], ğ‘¤ is the width of an element) 

2. äºŒç»´æ•°ç»„(row-major layout) ğ’ƒğ’‚ğ’”ğ’† + ğ’ŠğŸâˆ—ğ’˜ğŸ+ğ’ŠğŸâˆ—ğ’˜ğŸ(ğ‘¤1 is the width of a row, ğ‘¤2 is the width of an element)

   <img src="image-20250103165639434.png" alt="image-20250103165639434" style="zoom:25%;" />

   è¡Œä¼˜å…ˆå¸ƒå±€ï¼Œåˆ—ä¼˜å…ˆå¸ƒå±€






## Run-Time Environments (è¿è¡Œæ—¶åˆ»ç¯å¢ƒ)

å®ƒä¸»è¦æœ‰è¿™äº›åŠŸèƒ½:

Layout and allocation of storage locations for data in the source program

AND Mechanisms to access variables

AND Linkages between procedures, the mechanisms for passing parameters

###  Storage Organization

æ„æˆå¦‚ä¸‹

<img src="image-20250103171814194.png" alt="image-20250103171814194" style="zoom:25%;" />

1. Static: the storage-allocation decision can be made by the compiler by looking only at the program text

2. Dynamic: è¿è¡Œæ—¶å€™çš„å†³å®š

   Stack Storage ï¼ˆæ ˆåŒºï¼‰: The space for **names local** to a procedure is allocated on a stack. The lifetime of the data is the same as that of the called procedure Short-lived activation records

   Heap Storage (å †åŒº) ï¼šHold data that may outlive the call to the procedure that created itï¼ˆLong-lived dataï¼‰

   - Manual memory deallocaion æ‰‹åŠ¨å›æ”¶å†…å­˜
   - Automatic memory deallocation åƒåœ¾å›æ”¶



### Stack Space Allocation

1. æ´»åŠ¨æ ‘(activation tree)

   å¯¹äºä¸€ä¸ªæ´»åŠ¨æ ‘: Each node corresponds to one activation (children nodes are ordered) && The root is the activation of the â€œmainâ€ procedure

   FILO CALLs éµå¾ªpreorder RETURNs éµå¾ªpostorder

2. æ´»åŠ¨è®°å½•(activation record)

   Procedure calls and returns are usually managed by a run-time stack called the **control stack** (or call stack)

   Each live activation has an activation record (or stack frame) on the **control stack**

   ç›¸å½“äºå°±æ˜¯è®°å½•å“ªäº›activation treeä¸Šçš„nodeè¿˜æ˜¯activeçŠ¶æ€

   Activation Recordä¸­çš„ä¿¡æ¯ï¼šActual parameters, Returned values, Control link, Access link,  Saved machine status,  Local data, Temporaries

3. Calling Sequences(æ–¹æ³•è°ƒç”¨åºåˆ—)

   **Calling sequences**, consisting of code that 

   (1) allocates an activation record on the stack and (2) enters information into its fields

   **Return sequence (è¿”å›ä»£ç åºåˆ—)** restores the state of the machine so that the caller can continue its execution after the call

   ä»–ä»¬çš„ä½œç”¨ï¼š

   1. dataæ–¹é¢ Correctly pass arguments to the callee && Correctly pass the return values to the caller æ­£ç¡®çš„è¿›è¡Œäº†ä¼ å€¼
   2. controlæ–¹é¢ Correctly transfer the control to the first instruction of the callee && Correctly transfer the control back to the caller so that it can continue with the instruction immediately after the procedure-call statement å®Œæˆäº†æ§åˆ¶æƒçš„è½¬æ¢

   Calling å’Œ Return çš„å¯¹åº”çš„Step ï¼ˆæ­¤å¤„ç•¥ï¼‰



### Heap Management

å †ä¸­çš„åˆ†é…ä¾‹å¦‚Javaä¸­çš„newï¼ŒCä¸­çš„mallocï¼Œå¯ä»¥è¾ƒé•¿æ—¶é—´çš„live

deallocate ä¾‹å¦‚Javaä¸­çš„garbage collectorè‡ªåŠ¨å›æ”¶ï¼ŒCä¸­çš„free, delete

Properties: Space efficiency, Program efficiency, Low overhead

### Program Locality (ç¨‹åºå±€éƒ¨æ€§)

1. Temporal locality (æ—¶é—´å±€éƒ¨æ€§): the memory locations accessed are likely to  be accessed again within a short period of time
2. Spatial locality (ç©ºé—´å±€éƒ¨æ€§): memory locations close to the locations accessed are likely to be accessed within a short period of time



### Reducing Fragmentation(å‡å°‘å†…å­˜ç¢ç‰‡åŒ–)

ç¨‹åºå¼€å§‹æ—¶å€™Heapæ˜¯åœ¨ä¸€ä¸ªè¿ç»­çš„freeç©ºé—´ä¸Šçš„ï¼Œä½†æ˜¯éšç€allocate memoryçš„è¿›è¡Œthe memory manager must place the requested memory into a large-enough hole

æ¥ç€With each deallocation request, the freed memory are added back to the pool of free space 

ç¢ç‰‡åŒ–ç°è±¡ä¼šé™ä½æ•ˆç‡ ä¸‹é¢æœ‰å‡ ç§approaches

1. Best-fit algorithm: Allocate the requested memory in the smallest available hole that is large enough
2. First-fit algorithm: An object is placed in the first (lowest-address) hole in which it fits èŠ±è´¹æ›´å°‘çš„æ—¶é—´æé«˜spatial locality ä½†æ˜¯åœ¨æ•´ä½“è¡¨ç°ä¸Šå·®äºbest-fit
3. ä½†æ˜¯best-fitç®—æ³•éœ€è¦éå†æ¯ä¸ªå®¹å™¨ï¼Œæ•ˆç‡å¾ˆä½ï¼Œäºæ˜¯æå‡ºäº†å‡ ä¸ªstrategy
   - binning strategy åˆ†æˆ Separate bins
   - Doug Lea's strategy: aligns all chunks to 8-byte boundaries (i.e., chunk size is always a multiple of eight)





## Code Generationï¼ˆä»£ç ç”Ÿæˆï¼‰

TASKs:

1. Instruction selection 
2. Register allocation and assignment 
3. Instruction ordering



### Addresses in target code

1. Handling procedure calls and returns

   - Static allocation (é™æ€åˆ†é…) 

     Definition: The size and layout of activation records are determined by the code generator via the information in the symbol table

   - Stack allocation (æ ˆå¼åˆ†é…)

     æ ˆåˆ†é…æ˜¯æŒ‡å°†å†…å­˜åˆ†é…åœ¨ç¨‹åºè¿è¡Œæ—¶çš„**è°ƒç”¨æ ˆï¼ˆcall stackï¼‰**ä¸Šã€‚

     Static allocationä½¿ç”¨çš„æ˜¯ç»å¯¹åœ°å€ï¼Œè€Œstack allocationä½¿ç”¨çš„æ˜¯ç›¸å¯¹åœ°å€

     usually in activation record

2. Handling names

   å¯¹åº”çš„symbol-table entry



### Basic Blocks and Flow Graph

1. åŸºæœ¬å—åˆ‡åˆ†ç®—æ³•

   ä¸‰ä¸ªæ³•åˆ™éœ€è¦éµä»

   1. The first instruction in the entire intermediate code is a leader 
   2. Any instruction that is the target of a (un)conditional jump is a leader 
   3. Any instruction that immediately follows a (un)conditional jump is a leader

   æŒ‰ç…§è¿™æ ·å°†ä¸‰åœ°å€ç ç¨‹åºåˆ‡åˆ†æˆåŸºæœ¬å—

2. æ§åˆ¶æµå›¾æ„é€ ç®—æ³•

   å¯¹äºåˆšæ‰æ„é€ çš„blockå›¾ï¼Œæ·»åŠ è¾¹ï¼Œmeaningè·³è½¬å…³ç³»æˆ–è€…è‡ªç„¶è¿è¡Œå…³ç³»

   loopçš„å®šä¹‰åœ¨æ§åˆ¶æµå›¾ä¸­çš„ç¯



### Optimization of Basic Blocks

å‡ ç§ç­–ç•¥

1. æ„é€ DAG æ‰¾åˆ°Local subexpression(eliminating)ï¼Œå¯ä»¥è¿›è¡Œå¤ç”¨æˆ–è€…åˆå¹¶
2. Dead Code Elimination
3. The Use of Algebraic Identities ï¼ˆä»£æ•°æ’ç­‰å¼çš„åº”ç”¨ï¼‰



### Registers

Two important data structures:

1. Register descriptor (å¯„å­˜å™¨æè¿°ç¬¦):  For each available , keeping track of the variable names whose current  value is in that register 
2. Address descriptor (åœ°å€æè¿°ç¬¦): For each program variable,  keeping track of the locations where the current value of  that variable can be found (A location may be a register, a memory address, a stack location)

**Register Allocation**

Assign specific values to certain registers, which simplifies the design and implementation of a compiler 

However, inefficient uses of registers may occur. Certain registers may go unused, while many loads and stores are  generated for the other register

**Global Register Allocation**

To save stores and loads, we can assign registers to frequently used variables and keep these registers consistent across block 2boundaries (globally)



## Introduction to Data-Flow Analysis (Code optimization)

We associate with every program point a **data-flow value** that represents an abstraction of program states observed for that point. ç»™æ¯ä¸ªç¨‹åºä¸­çš„ä½ç½®ä¸€ä¸ªæ•°æ®æµå€¼ï¼Œä»£è¡¨ä¸€ä¸ªæŠ½è±¡çš„è§‚æµ‹å€¼

The data-flow problem is to find a solution to a set of constraints on  the IN[s]â€™s and OUT[s]â€™s for all statements 

1. Constraints based on the semantics of the statements (â€œtransfer functionsâ€) 
2. Constraints based on the flow of control

æ€»ä¹‹ï¼Œä¸ºäº†æ‰¾åˆ°å¯èƒ½æ”¹å˜æ•°æ®æµå€¼çš„æ­¥éª¤ä»¥åŠæ§åˆ¶å¯èƒ½ä¼ æ’­è¿™ä¸ªå€¼çš„flow



### Reaching definition

å¦‚æœå®šä¹‰(definition, assign, ...)dèƒ½åˆ°è¾¾æŸä¸ªç‚¹pè€Œä¸è¢«kill(æˆ–è€…è¢«åˆ«çš„æ­¥éª¤modify)ï¼Œé‚£ä¹ˆdæ˜¯å¯¹xçš„last definitionï¼Œç§°è¿™äº›æ­¥éª¤ä¸ºreaching definition

<img src="image-20250103191522102.png" alt="image-20250103191522102" style="zoom:25%;" />

æ ¹æ®æ¯ä¸ªblockçš„gen,killé›†åˆæ ¹æ®ä»¥ä¸Šç®—æ³•è®¡ç®—Out

æœ€åç»è¿‡å‡ ä¸ªiterä¹‹åæ²¡æœ‰æ›´æ–°äº†å°±åˆ°è¾¾äº†fixed point